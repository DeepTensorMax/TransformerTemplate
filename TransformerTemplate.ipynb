{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-4.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf\n",
    "\n",
    "#üython 3.11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully Imported All Modules'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "def doImport():\n",
    "    global pd, sys, cdb\n",
    "    try:\n",
    "        import sys\n",
    "        import pandas as pd\n",
    "        import chromadb as cdb # force install pydantic version to 1.9.0 !!! otherwise this will throw PydanticImportError\n",
    "        return \"Successfully Imported All Modules\"\n",
    "    except Exception:\n",
    "        # try to install missing modules\n",
    "        sys.stderr.write(\"Missing modules detected, please install according to readme...\\n\")\n",
    "    \n",
    "doImport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_DB = False\n",
    "\n",
    "model_name = \"mrm8488--t5-base-finetuned-question-generation-ap\"\n",
    "\n",
    "metadata_fields = [\"authors\", \"pubdate\"] # field names that contain metadata\n",
    "model_cache_dir = \"cache/\"\n",
    "data_dir = \"preprocessing/data/out\"\n",
    "firstNElements = 1000 # number of elements to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load every .txt file in data_dir in pandas dataframe with columns: \"text\", \"ID\". ID is the filename without extension | text is the text of the file\n",
    "def load_data(data_dir):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for filename in os.listdir(data_dir)[:firstNElements]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, filename), \"r\") as f:\n",
    "                data.append({\"text\": f.read(), \"ID\": filename[:-4]})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "data = load_data(data_dir)\n",
    "\n",
    "def create_example(doc1: pd.Series):\n",
    "    return InputExample(texts=[doc1])\n",
    "\n",
    "train_examples_texts = data.apply(lambda x: create_example(x[\"text\"]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 16.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 1000, 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_name,\n",
    "    cache_folder=model_cache_dir\n",
    ")\n",
    "\n",
    "text_embeddings = model.encode(data[\"text\"].tolist(), show_progress_bar=True)\n",
    "print(f\"Dimensions: {len(text_embeddings)}, {len(text_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "if PERSISTENT_DB:\n",
    "    db_dir = \"db/\"\n",
    "    db_name = \"chroma.db\"\n",
    "    db_path = db_dir + db_name\n",
    "    db = cdb.Client(cdb.config.Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=db_path))\n",
    "else:\n",
    "    chroma_client = cdb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Collection: pubmed\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"pubmed\"\n",
    "\n",
    "if(len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]):\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "else:\n",
    "    print(f\"Creating Collection: {collection_name}\")\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=data[\"text\"].tolist(),\n",
    "    # metadatas=[{metadata: datapoint} for metadata in metadata_fields for datapoint in data[metadata].tolist()],\n",
    "    ids=data[\"ID\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['34131359', '33352270', '36068652', '34276140', '33649773', '35407664', '36981511', '32993795', '37455915', '32356867']], 'embeddings': None, 'documents': [['40. New Gener Comput. 2021;39(3-4):717-741. doi: 10.1007/s00354-021-00128-0. Epub\\n 2021 Jun 10.\\n\\nLeveraging Artificial Intelligence (AI) Capabilities for COVID-19 Containment.\\n\\nSurianarayanan C(1), Chelliah PR(2).\\n\\nAuthor information:\\n(1)Government Arts and Science College (Formerly Bharathidasan University \\nConstituent Arts and Science College), Affiliated to Bharathidasan University, \\nTiruchirappalli, Tamilnadu India.\\n(2)Site Reliability Engineering Division, Reliance Jio Platforms Ltd, Bangalore, \\nIndia.\\n\\nThe Coronavirus disease (COVID-19) is an infectious disease caused by the newly \\ndiscovered Severe Acute Respiratory Syndrome Coronavirus two (SARS-CoV-2). Most \\nof the people do not have the acquired immunity to fight this virus. There is no \\nspecific treatment or medicine to cure the disease. The effects of this disease \\nappear to vary from individual to individual, right from mild cough, fever to \\nrespiratory disease. It also leads to mortality in many people. As the virus has \\na very rapid transmission rate, the entire world is in distress. The control and \\nprevention of this disease has evolved as an urgent and critical issue to be \\naddressed through technological solutions. The Healthcare industry therefore \\nneeds support from the domain of artificial intelligence (AI). AI has the \\ninherent capability of imitating the human brain and assisting in \\ndecision-making support by automatically learning from input data. It can \\nprocess huge amounts of data quickly without getting tiresome and making errors. \\nAI technologies and tools significantly relieve the burden of healthcare \\nprofessionals. In this paper, we review the critical role of AI in responding to \\ndifferent research challenges around the COVID-19 crisis. A sample \\nimplementation of a powerful probabilistic machine learning (ML) algorithm for \\nassessment of risk levels of individuals is incorporated in this paper. Other \\npertinent application areas such as surveillance of people and hotspots, \\nmortality prediction, diagnosis, prognostic assistance, drug repurposing and \\ndiscovery of protein structure, and vaccine are presented. The paper also \\ndescribes various challenges that are associated with the implementation of \\nAI-based tools and solutions for practical use.\\n\\n© Ohmsha, Ltd. and Springer Japan KK, part of Springer Nature 2021.\\n\\nDOI: 10.1007/s00354-021-00128-0\\nPMCID: PMC8191724\\nPMID: 34131359\\n\\nConflict of interest statement: Conflict of interestOn behalf of all authors, \\nthe corresponding author states that there is no conflict of interest.', '126. Int J Infect Dis. 2021 Mar;104:1-6. doi: 10.1016/j.ijid.2020.12.040. Epub\\n2020  Dec 19.\\n\\nThe collective wisdom in the COVID-19 research: Comparison and synthesis of \\nepidemiological parameter estimates in preprints and peer-reviewed articles.\\n\\nWang Y(1), Cao Z(2), Zeng DD(3), Zhang Q(4), Luo T(1).\\n\\nAuthor information:\\n(1)Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; \\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, \\nBeijing 100049, China.\\n(2)Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. \\nElectronic address: zhidong.cao@ia.ac.cn.\\n(3)Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.\\n(4)City University of Hong Kong, Hong Kong 999077, China.\\n\\nOBJECTIVES: We aimed to explore the collective wisdom of preprints related to \\nCOVID-19 by comparing and synthesizing them with results of peer-reviewed \\npublications.\\nMETHODS: PubMed, Google Scholar, medRxiv, bioRxiv, arXiv, and SSRN were searched \\nfor papers regarding the estimation of four epidemiological parameters of \\nCOVID-19: the basic reproduction number, incubation period, infectious period, \\nand case-fatality-rate. Distributions of parameters and timeliness of preprints \\nand peer-reviewed papers were compared. Four parameters in two groups were \\nsynthesized by bootstrapping, and their validities were evaluated by simulated \\ncumulative cases of the susceptible-exposed-infectious-recovered-dead-cumulative \\n(SEIRDC) model.\\nRESULTS: A total of 106 papers were included for analysis. The distributions of \\nfour parameters in two literature groups were close, and the timeliness of \\npreprints was better. Synthesized estimates of the basic reproduction number \\n(3.18, 95% CI 2.85-3.53), incubation period (5.44 days, 95% CI 4.98-5.99), \\ninfectious period (6.25 days, 95% CI 5.09-7.51), and case-fatality-rate (4.51%, \\n95% CI 3.41%-6.29%) were obtained. Simulated cumulative cases of the SEIRDC \\nmodel matched well with the onset cases in China.\\nCONCLUSIONS: The validity of the COVID-19 parameter estimations of the preprints \\nwas on par with that of peer-reviewed publications, and synthesized results of \\nliteratures could reduce the uncertainty and be used for epidemic \\ndecision-making.\\n\\nCopyright © 2020 The Author(s). Published by Elsevier Ltd.. All rights reserved.\\n\\nDOI: 10.1016/j.ijid.2020.12.040\\nPMID: 33352327 [Indexed for MEDLINE]', '76. Comput Electr Eng. 2022 Oct;103:108352. doi:\\n10.1016/j.compeleceng.2022.108352.  Epub 2022 Sep 2.\\n\\nAn AI-based disease detection and prevention scheme for COVID-19.\\n\\nTanwar S(1), Kumari A(1), Vekaria D(1), Kumar N(2)(3), Sharma R(4).\\n\\nAuthor information:\\n(1)Department of Computer Science and Engineering, Institute of Technology, \\nNirma University, Ahmedabad, India.\\n(2)Thapar Institute of Engineering and Technology, (Deemed to be University), \\nPatiala, Punjab, India.\\n(3)Department of Computer Science and Information Engineering, Asia University, \\nTaichung, Taiwan.\\n(4)Centre for Inter-Disciplinary Research and Innovation, University of \\nPetroleum and Energy Studies, P.O. Bidholi Via-Prem Nagar, Dehradun, India.\\n\\nThe proliferating outbreak of COVID-19 raises global health concerns and has \\nbrought many countries to a standstill. Several restrain strategies are imposed \\nto suppress and flatten the mortality curve, such as lockdowns, quarantines, \\netc. Artificial Intelligence (AI) techniques could be a promising solution to \\nleverage these restraint strategies. However, real-time decision-making \\nnecessitates a cloud-oriented AI solution to control the pandemic. Though many \\ncloud-oriented solutions exist, they have not been fully exploited for real-time \\ndata accessibility and high prediction accuracy. Motivated by these facts, this \\npaper proposes a cloud-oriented AI-based scheme referred to as D-espy (i.e., \\nDisease-espy) for disease detection and prevention. The proposed D-espy scheme \\nperforms a comparative analysis between Autoregressive Integrated Moving Average \\n(ARIMA), Vanilla Long Short Term Memory (LSTM), and Stacked LSTM techniques, \\nwhich signify the dominance of Stacked LSTM in terms of prediction accuracy. \\nThen, a Medical Resource Distribution (MRD) mechanism is proposed for the \\noptimal distribution of medical resources. Next, a three-phase analysis of the \\nCOVID-19 spread is presented, which can benefit the governing bodies in deciding \\nlockdown relaxation. Results show the efficacy of the D-espy scheme concerning \\n96.2% of prediction accuracy compared to the existing approaches.\\n\\n© 2022 Elsevier Ltd. All rights reserved.\\n\\nDOI: 10.1016/j.compeleceng.2022.108352\\nPMCID: PMC9436917\\nPMID: 36068837\\n\\nConflict of interest statement: No author associated with this paper has \\ndisclosed any potential or pertinent conflicts which may be perceived to have \\nimpending conflict with this work. For full disclosure statements refer to \\nhttps://doi.org/10.1016/j.compeleceng.2022.108352.', \"25. Multimed Syst. 2022;28(4):1189-1222. doi: 10.1007/s00530-021-00818-1. Epub\\n2021  Jul 13.\\n\\nFusion of AI techniques to tackle COVID-19 pandemic: models, incidence rates, \\nand future trends.\\n\\nShah H(1), Shah S(1), Tanwar S(1), Gupta R(1), Kumar N(2)(3)(4).\\n\\nAuthor information:\\n(1)Department of Computer Science and Engineering, Institute of Technology, \\nNirma University, Ahmedabad, India.\\n(2)Department of Computer Science Engineering, Thapar Institute of Engineering \\nand Technology, Deemed to be University, Patiala, India.\\n(3)School of Computer Science, University of Petroleum and Energy Studies, \\nDehradun, Uttarakhand India.\\n(4)King Abdul Aziz University, Jeddah, Saudi Arabia.\\n\\nThe COVID-19 pandemic is rapidly spreading across the globe and infected \\nmillions of people that take hundreds of thousands of lives. Over the years, the \\nrole of Artificial intelligence (AI) has been on the rise as its algorithms are \\ngetting more and more accurate and it is thought that its role in strengthening \\nthe existing healthcare system will be the most profound. Moreover, the pandemic \\nbrought an opportunity to showcase AI and healthcare integration potentials as \\nthe current infrastructure worldwide is overwhelmed and crumbling. Due to AI's \\nflexibility and adaptability, it can be used as a tool to tackle COVID-19. \\nMotivated by these facts, in this paper, we surveyed how the AI techniques can \\nhandle the COVID-19 pandemic situation and present the merits and demerits of \\nthese techniques. This paper presents a comprehensive end-to-end review of all \\nthe AI-techniques that can be used to tackle all areas of the pandemic. Further, \\nwe systematically discuss the issues of the COVID-19, and based on the \\nliterature review, we suggest their potential countermeasures using AI \\ntechniques. In the end, we analyze various open research issues and challenges \\nassociated with integrating the AI techniques in the COVID-19.\\n\\n© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part \\nof Springer Nature 2021.\\n\\nDOI: 10.1007/s00530-021-00818-1\\nPMCID: PMC8275905\\nPMID: 34276140\", \"50. J Infect Dis. 2021 Oct 28;224(8):1362-1371. doi: 10.1093/infdis/jiab107.\\n\\nPersistent SARS-CoV-2 RNA Shedding Without Evidence of Infectiousness: A Cohort \\nStudy of Individuals With COVID-19.\\n\\nOwusu D(1)(2), Pomeroy MA(1)(2), Lewis NM(1)(3), Wadhwa A(1)(4), Yousaf \\nAR(1)(2), Whitaker B(2), Dietrich E(2), Hall AJ(2), Chu V(1)(2), Thornburg N(2), \\nChristensen K(5), Kiphibane T(6), Willardson S(7), Westergaard R(8), Dasu T(9), \\nPray IW(1)(8), Bhattacharyya S(9), Dunn A(3), Tate JE(2), Kirking HL(2), \\nMatanock A(2); Household Transmission Study Team.\\n\\nCollaborators: Duca LM, Rabold E, Gharpure R, Njuguna H, Dawson P, Conners EE, \\nFields V, Salvatore P, Marcenac P, Reses HE, Fajans M, Laws RL, Yin S, Ye D, \\nPevzner E, Battey K, Tran C, O'Hegarty M, Vuong J, Chancey RJ, Gregory CJ, Banks \\nM, Rispens J, Lester S, Mills L, Fry A, Nabity S, Freeman B, Buono S.\\n\\nAuthor information:\\n(1)Epidemic Intelligence Service, Centers for Disease Control and Prevention, \\nAtlanta, Georgia, USA.\\n(2)COVID-19 Response Team, Centers for Disease Control and Prevention, Atlanta, \\nGeorgia, USA.\\n(3)Utah Department of Health, Salt Lake City, Utah, USA.\\n(4)Laboratory Leadership Service, Centers for Disease Control and Prevention, \\nAtlanta, Georgia, USA.\\n(5)Utah Public Health Laboratory, Taylorsville, Utah, USA.\\n(6)Salt Lake County Health Department, Salt Lake City, Utah, USA.\\n(7)Davis County Health Department, Clearfield, Utah, USA.\\n(8)Wisconsin Department of Health Services, Madison, Wisconsin, USA.\\n(9)City of Milwaukee Health Department, Milwaukee, Wisconsin, USA.\\n\\nBACKGROUND: To better understand severe acute respiratory syndrome coronavirus 2 \\n(SARS-CoV-2) shedding and infectivity, we estimated SARS-CoV-2 RNA shedding \\nduration, described participant characteristics associated with the first \\nnegative rRT-PCR test (resolution), and determined if replication-competent \\nviruses was recoverable ≥10 days after symptom onset.\\nMETHODS: We collected serial nasopharyngeal specimens from 109 individuals with \\nrRT-PCR-confirmed COVID-19 in Utah and Wisconsin. We calculated viral RNA \\nshedding resolution probability using the Kaplan-Meier estimator and evaluated \\ncharacteristics associated with shedding resolution using Cox proportional \\nhazards regression. We attempted viral culture for 35 rRT-PCR-positive \\nnasopharyngeal specimens collected ≥10 days after symptom onset.\\nRESULTS: The likelihood of viral RNA shedding resolution at 10 days after \\nsymptom onset was approximately 3%. Time to shedding resolution was shorter \\namong participants aged <18 years (adjusted hazards ratio [aHR], 3.01; 95% \\nconfidence interval [CI], 1.6-5.6) and longer among those aged ≥50 years (aHR, \\n0.50; 95% CI, .3-.9) compared to participants aged 18-49 years. No \\nreplication-competent viruses were recovered.\\nCONCLUSIONS: Although most patients were positive for SARS-CoV-2 for ≥10 days \\nafter symptom onset, our findings suggest that individuals with mild to moderate \\nCOVID-19 are unlikely to be infectious ≥10 days after symptom onset.\\n\\nPublished by Oxford University Press for the Infectious Diseases Society of \\nAmerica 2021.\\n\\nDOI: 10.1093/infdis/jiab107\\nPMCID: PMC7989388\\nPMID: 33649773 [Indexed for MEDLINE]\", '9. J Clin Med. 2022 Apr 6;11(7):2054. doi: 10.3390/jcm11072054.\\n\\nArtificial Intelligence for COVID-19 Detection in Medical Imaging-Diagnostic \\nMeasures and Wasting-A Systematic Umbrella Review.\\n\\nJemioło P(1), Storman D(2), Orzechowski P(1)(3).\\n\\nAuthor information:\\n(1)AGH University of Science and Technology, Faculty of Electrical Engineering, \\nAutomatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, \\n30-059 Krakow, Poland.\\n(2)Chair of Epidemiology and Preventive Medicine, Department of Hygiene and \\nDietetics, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 \\nKrakow, Poland.\\n(3)Institute for Biomedical Informatics, University of Pennsylvania, 3700 \\nHamilton Walk, Philadelphia, PA 19104, USA.\\n\\nThe COVID-19 pandemic has sparked a barrage of primary research and reviews. We \\ninvestigated the publishing process, time and resource wasting, and assessed the \\nmethodological quality of the reviews on artificial intelligence techniques to \\ndiagnose COVID-19 in medical images. We searched nine databases from inception \\nuntil 1 September 2020. Two independent reviewers did all steps of \\nidentification, extraction, and methodological credibility assessment of \\nrecords. Out of 725 records, 22 reviews analysing 165 primary studies met the \\ninclusion criteria. This review covers 174,277 participants in total, including \\n19,170 diagnosed with COVID-19. The methodological credibility of all eligible \\nstudies was rated as critically low: 95% of papers had significant flaws in \\nreporting quality. On average, 7.24 (range: 0-45) new papers were included in \\neach subsequent review, and 14% of studies did not include any new paper into \\nconsideration. Almost three-quarters of the studies included less than 10% of \\navailable studies. More than half of the reviews did not comment on the \\npreviously published reviews at all. Much wasting time and resources could be \\navoided if referring to previous reviews and following methodological \\nguidelines. Such information chaos is alarming. It is high time to draw \\nconclusions from what we experienced and prepare for future pandemics.\\n\\nDOI: 10.3390/jcm11072054\\nPMCID: PMC9000039\\nPMID: 35407664\\n\\nConflict of interest statement: The authors declare no conflict of interest. The \\nfunders had no role in the design of the study; in the collection, analyses, or \\ninterpretation of data; in the writing of the manuscript, or in the decision to \\npublish the results.', \"57. Healthcare (Basel). 2023 Mar 14;11(6):854. doi: 10.3390/healthcare11060854.\\n\\nArtificial Intelligence: A Next-Level Approach in Confronting the COVID-19 \\nPandemic.\\n\\nMahalakshmi V(1), Balobaid A(1), Kanisha B(2), Sasirekha R(3), Ramkumar Raja \\nM(4).\\n\\nAuthor information:\\n(1)Department of Computer Science, College of Computer Science & Information \\nTechnology, Jazan University, Jazan 45142, Saudi Arabia.\\n(2)Department of Computer Science and Engineering, School of Computing, College \\nof Engineering and Technology, SRM Institute of Science and Technology, \\nChengalpattu 603203, India.\\n(3)Department of Computing Technologies, SRM Institute of Science and \\nTechnology, Kattankulathur Campus, Chengalpattu 603203, India.\\n(4)Department of Electrical Engineering, College of Engineering, King Khalid \\nUniversity, Abha 62529, Saudi Arabia.\\n\\nThe severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) which caused \\ncoronavirus diseases (COVID-19) in late 2019 in China created a devastating \\neconomical loss and loss of human lives. To date, 11 variants have been \\nidentified with minimum to maximum severity of infection and surges in cases. \\nBacterial co-infection/secondary infection is identified during viral \\nrespiratory infection, which is a vital reason for morbidity and mortality. The \\noccurrence of secondary infections is an additional burden to the healthcare \\nsystem; therefore, the quick diagnosis of both COVID-19 and secondary infections \\nwill reduce work pressure on healthcare workers. Therefore, well-established \\nsupport from Artificial Intelligence (AI) could reduce the stress in healthcare \\nand even help in creating novel products to defend against the coronavirus. AI \\nis one of the rapidly growing fields with numerous applications for the \\nhealthcare sector. The present review aims to access the recent literature on \\nthe role of AI and how its subfamily machine learning (ML) and deep learning \\n(DL) are used to curb the pandemic's effects. We discuss the role of AI in \\nCOVID-19 infections, the detection of secondary infections, technology-assisted \\nprotection from COVID-19, global laws and regulations on AI, and the impact of \\nthe pandemic on public life.\\n\\nDOI: 10.3390/healthcare11060854\\nPMCID: PMC10048108\\nPMID: 36981511\\n\\nConflict of interest statement: The authors declare no conflict of interest.\", \"94. Eur Urol. 2020 Dec;78(6):775-776. doi: 10.1016/j.eururo.2020.09.031. Epub\\n2020  Sep 17.\\n\\nThe Emerging Role of Artificial Intelligence in the Fight Against COVID-19.\\n\\nGhose A(1), Roy S(2), Vasdev N(3), Olsburgh J(4), Dasgupta P(5).\\n\\nAuthor information:\\n(1)Medway NHS Foundation Trust, Gillingham, UK.\\n(2)Applied Intelligence, Accenture, London, UK.\\n(3)Department of Urology, Hertfordshire and Bedfordshire Urological Cancer \\nCentre, Lister Hospital Stevenage, School of Medicine and Life Sciences, \\nUniversity of Hertfordshire, Hatfield, UKE.\\n(4)Guy's and St. Thomas' NHS Foundation Trust, London, UK.\\n(5)Faculty of Life Sciences and Medicine, King's College London, London, UK. \\nElectronic address: prokar.dasgupta@kcl.ac.uk.\\n\\nThe coronavirus disease 2019 (COVID-19) pandemic has generated large volumes of \\nclinical data that can be an invaluable resource towards answering a number of \\nimportant questions for this and future pandemics. Artificial intelligence can \\nhave an important role in analysing such data to identify populations at higher \\nrisk of COVID-19-related urological pathologies and to suggest treatments that \\nblock viral entry into cells by interrupting the angiotensin-converting enzyme \\n2-transmembrane serine protease 2 (ACE2-TMPRSS2) pathway.\\n\\nCopyright © 2020 European Association of Urology. Published by Elsevier B.V. All \\nrights reserved.\\n\\nDOI: 10.1016/j.eururo.2020.09.031\\nPMCID: PMC7498248\\nPMID: 32994064 [Indexed for MEDLINE]\", \"191. Heliyon. 2023 Jun 30;9(7):e17865. doi: 10.1016/j.heliyon.2023.e17865. \\neCollection 2023 Jul.\\n\\nProlonged school closure during the pandemic time in successive waves of \\nCOVID-19- vulnerability of children to sexual abuses - A case study in Tamil \\nNadu, India.\\n\\nParamasivan K(1), Raj B(2)(3), Sudarasanam N(1)(4), Subburaj R(5).\\n\\nAuthor information:\\n(1)Department of Management Studies, Indian Institute of Technology, Madras @ \\nChennai, India.\\n(2)School of Computer Science, Carnegie Mellon University, Pittsburgh, USA.\\n(3)Mohammed Bin Zayed University of AI, Abu Dhabi, United Arab Emirates.\\n(4)Robert Bosch Center for Data Science and Artificial Intelligence, Indian \\nInstitute of Technology, Madras @ Chennai, India.\\n(5)Senior Data Scientist, Ford Motor Company, Chennai, India.\\n\\nOBJECTIVES: The Tamil Nadu government mandated several stay-at-home orders, with \\nrestrictions of varying intensities, to contain the first two waves of the \\nCOVID-19 pandemic. This research investigates how such orders impacted child \\nsexual abuse (CSA) by using counterfactual prediction to compare CSA statistics \\nwith those of other crimes. After adjusting for mobility, we investigate the \\nrelationship between situational factors and recorded levels of cases registered \\nunder the Protection of Children from Sexual Offences Act (POCSO). The \\nsituational factors include the victims' living environment, their access to \\nrelief agencies, and the competence and responsiveness of the police.\\nMETHODS: We adopt an auto-regressive neural network method to make a \\ncounterfactual forecast of CSA cases that represents a scenario without \\nstay-at-home orders, relying on the eight-year daily count data of POCSO cases \\nin Tamil Nadu. Using the insights from Google's COVID-19 Community Mobility \\nReports, we measure changes in mobility across various community spaces during \\nthe various phases of stay-at-home orders in both waves in 2020 and 2021.\\nRESULTS: The steep falls in POCSO cases during strict stay-at-home periods, \\ncompared with the counterfactual estimates, were -72% (Cliff's delta -0.99) and \\n-36% (Cliff's delta -0.65) during the first and second waves, respectively. \\nHowever, in the post-lockdown phases, there were sharp increases of 68% (Cliff's \\ndelta 0.65) and 36% (Cliff's delta 0.56) in CSA cases during the first and \\nsecond waves, with concomitantly quicker reporting of case registration.\\nCONCLUSIONS: Considering that the median delay in filing CSA complaints was \\nabove 30 days in the mild and post-intervention periods, the upsurge of cases in \\nthe more relaxed phases indicates increased occurrences of CSA during strict \\nlockdowns. Overall, higher victimization numbers were observed during the \\nprolonged lockdown-induced school closures. Our findings highlight the time gap \\nbetween the incidents and their registration during the strict lockdown phases.\\n\\n© 2023 The Authors.\\n\\nDOI: 10.1016/j.heliyon.2023.e17865\\nPMCID: PMC10339019\\nPMID: 37456023\\n\\nConflict of interest statement: The authors declare that they have no known \\ncompeting financial interests or personal relationships that could have appeared \\nto influence the work reported in this paper.\", '7. JAMA Intern Med. 2020 Sep 1;180(9):1156-1163. doi: \\n10.1001/jamainternmed.2020.2020.\\n\\nContact Tracing Assessment of COVID-19 Transmission Dynamics in Taiwan and Risk \\nat Different Exposure Periods Before and After Symptom Onset.\\n\\nCheng HY(1), Jian SW(1), Liu DP(1), Ng TC(2), Huang WT(3), Lin HH(2)(4); Taiwan \\nCOVID-19 Outbreak Investigation Team.\\n\\nAuthor information:\\n(1)Epidemic Intelligence Center, Taiwan Centers for Disease Control, Taipei, \\nTaiwan.\\n(2)Institute of Epidemiology and Preventive Medicine, National Taiwan University \\nCollege of Public Health, Taipei, Taiwan.\\n(3)Office of Preventive Medicine, Taiwan Centers for Disease Control, Taipei, \\nTaiwan.\\n(4)Global Health Program, National Taiwan University College of Public Health, \\nTaipei, Taiwan.\\n\\nErratum in\\n    JAMA Intern Med. 2020 Sep 1;180(9):1264.\\n\\nComment in\\n    JAMA Intern Med. 2020 Sep 1;180(9):1163-1164.\\n    JAMA Intern Med. 2020 Sep 1;180(9):1262.\\n\\nIMPORTANCE: The dynamics of coronavirus disease 2019 (COVID-19) transmissibility \\nare yet to be fully understood. Better understanding of the transmission \\ndynamics is important for the development and evaluation of effective control \\npolicies.\\nOBJECTIVE: To delineate the transmission dynamics of COVID-19 and evaluate the \\ntransmission risk at different exposure window periods before and after symptom \\nonset.\\nDESIGN, SETTING, AND PARTICIPANTS: This prospective case-ascertained study in \\nTaiwan included laboratory-confirmed cases of COVID-19 and their contacts. The \\nstudy period was from January 15 to March 18, 2020. All close contacts were \\nquarantined at home for 14 days after their last exposure to the index case. \\nDuring the quarantine period, any relevant symptoms (fever, cough, or other \\nrespiratory symptoms) of contacts triggered a COVID-19 test. The final follow-up \\ndate was April 2, 2020.\\nMAIN OUTCOMES AND MEASURES: Secondary clinical attack rate (considering \\nsymptomatic cases only) for different exposure time windows of the index cases \\nand for different exposure settings (such as household, family, and health \\ncare).\\nRESULTS: We enrolled 100 confirmed patients, with a median age of 44 years \\n(range, 11-88 years), including 44 men and 56 women. Among their 2761 close \\ncontacts, there were 22 paired index-secondary cases. The overall secondary \\nclinical attack rate was 0.7% (95% CI, 0.4%-1.0%). The attack rate was higher \\namong the 1818 contacts whose exposure to index cases started within 5 days of \\nsymptom onset (1.0% [95% CI, 0.6%-1.6%]) compared with those who were exposed \\nlater (0 cases from 852 contacts; 95% CI, 0%-0.4%). The 299 contacts with \\nexclusive presymptomatic exposures were also at risk (attack rate, 0.7% [95% CI, \\n0.2%-2.4%]). The attack rate was higher among household (4.6% [95% CI, \\n2.3%-9.3%]) and nonhousehold (5.3% [95% CI, 2.1%-12.8%]) family contacts than \\nthat in health care or other settings. The attack rates were higher among those \\naged 40 to 59 years (1.1% [95% CI, 0.6%-2.1%]) and those aged 60 years and older \\n(0.9% [95% CI, 0.3%-2.6%]).\\nCONCLUSIONS AND RELEVANCE: In this study, high transmissibility of COVID-19 \\nbefore and immediately after symptom onset suggests that finding and isolating \\nsymptomatic patients alone may not suffice to contain the epidemic, and more \\ngeneralized measures may be required, such as social distancing.\\n\\nDOI: 10.1001/jamainternmed.2020.2020\\nPMCID: PMC7195694\\nPMID: 32356867 [Indexed for MEDLINE]\\n\\nConflict of interest statement: Conflict of Interest Disclosures: None reported.']], 'metadatas': [[None, None, None, None, None, None, None, None, None, None]], 'distances': [[0.9296808242797852, 0.9774729609489441, 1.004711627960205, 1.028862476348877, 1.0629936456680298, 1.077007532119751, 1.0908126831054688, 1.0982049703598022, 1.10658597946167, 1.1251286268234253]]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 619/619 [00:00<00:00, 7.38MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 10.9MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 11.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 2.89MB/s]\n",
      "added_tokens.json: 100%|██████████| 4.04k/4.04k [00:00<00:00, 29.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 357/357 [00:00<00:00, 3.43MB/s]\n",
      "config.json: 100%|██████████| 930/930 [00:00<00:00, 6.88MB/s]\n",
      "pytorch_model.bin:  34%|███▎      | 8.16G/24.2G [12:04<23:23, 11.4MB/s]"
     ]
    }
   ],
   "source": [
    "# naive q a system\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# model gpt would prefer: gpt2-xl\n",
    "\n",
    "# model = \"gpt2-xl\"\n",
    "model = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model,\n",
    "                                          cache_dir=model_cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model,\n",
    "                                             cache_dir=model_cache_dir)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best treatment for covid?\"\n",
    "results = collection.query(query_texts=[question])\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n\\n\\n\\nThe user's question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Relevant context: #161. BMJ Open. 2021 Jan 22;11(1):e043012. doi: 10.1136/bmjopen-2020-043012.\\n\\nResearch Evaluation Alongside Clinical Treatment in COVID-19 (REACT COVID-19): \\nan observational and biobanking study.\\n\\nBurke H(1)(2), Freeman A(3)(2), Dushianthan A(1)(2), Celinski M(1)(2), Batchelor \\nJ(1)(2)(4)(5), Phan H(1)(5)(6), Borca F(4)(6), Kipps C(1)(2)(4)(6), Thomas \\nGJ(7), Faust SN(1)(2)(4)(6)(8), Sheard N(2), Williams S(2), Fitzpatrick P(9), \\nLanders D(9), Wilkinson T(1)(2).\\n\\nAuthor information:\\n(1)Faculty of Medicine, University of Southampton, Southampton, UK.\\n(2)University Hospital Southampton NHS Foundation Trust, Southampton, \\nSouthampton, UK.\\n(3)Faculty of Medicine, University of Southampton, Southampton, UK \\na.freeman@soton.ac.uk.\\n(4)Institute for Life Sciences, University of Southampton, Southampton, UK.\\n(5)Clinical Informatics Research Unit, Faculty of Medicine, University of \\nSouthampton, Southampton, UK.\\n(6)NIHR Southampton Biomedical Research Centre, University Hospital Southampton \\nNHS Foundation Trust, Southampton, UK.\\n(7)Cancer Sciences Unit, Faculty of Medicine, University of Southampton, UK.\\n(8)NIHR Southampton Clinical Research Facility, University Hospital Southampton \\nNHS Foundation Trust, Southampton, UK.\\n(9)University of Manchester, Cancer Biomarker Centre, Cancer Research UK \\nManchester Institute, Manchester, UK.\\n\\nINTRODUCTION: The COVID-19 pandemic caused by SARS-CoV-2 places immense \\nworldwide demand on healthcare services. Earlier identification of patients at \\nrisk of severe disease may allow intervention with experimental targeted \\ntreatments, mitigating the course of their disease and reducing critical care \\nservice demand.\\nMETHODS AND ANALYSIS: This prospective observational study of patients tested or \\ntreated for SARS-CoV-2, who are under the care of the tertiary University \\nHospital Southampton NHS Foundation Trust (UHSFT), captured data from admission \\nto discharge; data collection commenced on 7 March 2020. Core demographic and \\nclinical information, as well as results of disease-defining characteristics, \\nwas captured and recorded electronically from hospital clinical record systems \\nat the point of testing. Manual data were collected and recorded by the clinical \\nresearch team for assessments which are not part of the structured electronic \\nhealthcare record, for example, symptom onset date. Thereafter, participant \\nrecords were continuously updated during hospital stay and their follow-up \\nperiod. Participants aged >16 years were given the opportunity to provide \\nconsent for excess clinical sample storage with optional further biological \\nsampling. These anonymised samples were linked to the clinical data in the \\nReal-time Analytics for Clinical Trials platform and were stored within a \\nbiorepository at UHSFT.\\nETHICS AND DISSEMINATION: Ethical approval was obtained from the HRA Specific \\nReview Board (REC 20/HRA/2986) for waiver of informed consent for the \\ndatabase-only cohort; the procedures conform with the Declaration of Helsinki. \\nThe study design, protocol and patient-facing documentation for the biobanking \\narm of the study have been approved by North West Research Ethics Committee (REC \\n17/NW/0632) as an amendment to the National Institute for Health Research \\nSouthampton Clinical Research Facility-managed Southampton Research \\nBiorepository. This study will be published as peer-reviewed articles and \\npresented at conferences, presentations and workshops.\\n\\n© Author(s) (or their employer(s)) 2021. Re-use permitted under CC BY-NC. No \\ncommercial re-use. See rights and permissions. Published by BMJ.\\n\\nDOI: 10.1136/bmjopen-2020-043012\\nPMCID: PMC7830323\\nPMID: 33483446 [Indexed for MEDLINE]\\n\\nConflict of interest statement: Competing interests: None declared. #157. Nutrients. 2021 May 21;13(6):1760. doi: 10.3390/nu13061760.\\n\\nCalcifediol Treatment and Hospital Mortality Due to COVID-19: A Cohort Study.\\n\\nAlcala-Diaz JF(1)(2), Limia-Perez L(1), Gomez-Huelgas R(3)(4), Martin-Escalante \\nMD(5), Cortes-Rodriguez B(6), Zambrana-Garcia JL(7), Entrenas-Castillo M(8), \\nPerez-Caballero AI(1)(2), López-Carmona MD(3)(4), Garcia-Alegria J(5)(9), Lozano \\nRodríguez-Mancheño A(6), Arenas-de Larriva MDS(8), Pérez-Belmonte LM(3)(4), \\nJungreis I(10)(11), Bouillon R(12), Quesada-Gomez JM(13), Lopez-Miranda J(1)(2).\\n\\nAuthor information:\\n(1)Internal Medicine Department, IMIBIC/Reina Sofia University \\nHospital/University of Córdoba, Avda. Menéndez Pidal s/n, 14004 Córdoba, Spain.\\n(2)CIBER Fisiopatologia Obesidad y Nutrición (CIBEROBN), Instituto de Salud \\nCarlos III, 28029 Madrid, Spain.\\n(3)Internal Medicine Department, Regional University Hospital of Málaga, Avenida \\nde Carlos Haya, s/n, 29010 Málaga, Spain.\\n(4)Biomedical Research Institute of Málaga (IBIMA), University of Málaga (UMA), \\nAvenida de Carlos Haya, s/n, 29010 Málaga, Spain.\\n(5)Internal Medicine Department, Hospital Costa del Sol, Agencia Sanitaria Costa \\ndel Sol, 29603 Marbella, Málaga, Spain.\\n(6)Internal Medicine Department, Alto Guadalquivir Hospital, Andújar, 23740 \\nJaén, Spain.\\n(7)Internal Medicine Department, Hospital de Montilla, Agencia Sanitaria Alto \\nGuadalquivir, 14550 Córdoba, Spain.\\n(8)Pneumology Department, Reina Sofia University Hopital. Avda, Menendez Pidal \\ns/n, 14004 Córdoba, Spain.\\n(9)Department of Medicine, University of Málaga (UMA), Red de Investigación en \\nServicios de Salud en Enfermedades Crónicas (REDISSEC), 29071 Málaga, Spain.\\n(10)MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA \\n02139, USA.\\n(11)Broad Institute of MIT and Harvard, Cambridge, MA 02142, USA.\\n(12)Laboratory of Clinical and Experimental Endocrinology, Department of Chronic \\nDiseases, Metabolism and Ageing, KU Leuven, Herestraat, ON 1/902, 3000 Leuven, \\nBelgium.\\n(13)IMIBIC. CIBER de Fragilidad y Envejecimiento Saludable, Hospital \\nUniversitario Reina Sofía, Universidad de Córdoba, Fundación Progreso y Salud, \\nAvda. Menéndez Pidal s/n, 14004 Córdoba, Spain.\\n\\nCONTEXT: Calcifediol has been proposed as a potential treatment for COVID-19 \\npatients.\\nOBJECTIVE: To compare the administration or not of oral calcifediol on mortality \\nrisk of patients hospitalized because of COVID-19.\\nDESIGN: Retrospective, multicenter, open, non-randomized cohort study.\\nSETTINGS: Hospitalized care.\\nPATIENTS: Patients with laboratory-confirmed COVID-19 between 5 February and 5 \\nMay 2020 in five hospitals in the South of Spain.\\nINTERVENTION: Patients received calcifediol (25-hydroxyvitamin D3) treatment \\n(0.266 mg/capsule, 2 capsules on entry and then one capsule on day 3, 7, 14, 21, \\nand 28) or not.\\nMAIN OUTCOME MEASURE: In-hospital mortality during the first 30 days after \\nadmission.\\nRESULTS: A total of 537 patients were hospitalized with COVID-19 (317 males \\n(59%), median age, 70 years), and 79 (14.7%) received calcifediol treatment. \\nOverall, in-hospital mortality during the first 30 days was 17.5%. The OR of \\ndeath for patients receiving calcifediol (mortality rate of 5%) was 0.22 (95% \\nCI, 0.08 to 0.61) compared to patients not receiving such treatment (mortality \\nrate of 20%; p < 0.01). Patients who received calcifediol after admission were \\nmore likely than those not receiving treatment to have comorbidity and a lower \\nrate of CURB-65 score for pneumonia severity ≥ 3 (one point for each of \\nconfusion, urea > 7 mmol/L, respiratory rate ≥ 30/min, systolic blood pressure < \\n90 mm Hg or diastolic blood pressure ≤ 60 mm Hg, and age ≥ 65 years), acute \\nrespiratory distress syndrome (moderate or severe), c-reactive protein, chronic \\nkidney disease, and blood urea nitrogen. In a multivariable logistic regression \\nmodel, adjusting for confounders, there were significant differences in \\nmortality for patients receiving calcifediol compared with patients not \\nreceiving it (OR = 0.16 (95% CI 0.03 to 0.80).\\nCONCLUSION: Among patients hospitalized with COVID-19, treatment with \\ncalcifediol, compared with those not receiving calcifediol, was significantly \\nassociated with lower in-hospital mortality during the first 30 days. The \\nobservational design and sample size may limit the interpretation of these \\nfindings.\\n\\nDOI: 10.3390/nu13061760\\nPMCID: PMC8224356\\nPMID: 34064175 [Indexed for MEDLINE]\\n\\nConflict of interest statement: JFAD received lecture fees from Bayer, \\nGrunenthal Pharma, Esteve, Ferrer, and Boehringer Ingelheim outside the \\nsubmitted work. LLP received lecture fees from Gebro Pharma S.A., Boehringer \\nIngelheim, Pfizer, Mylan, Almirall, SANOFI, and ESTEVE outside the submitted \\nwork. IJ, RGH, MDME, BCR, JLZG, MEC, AIPC, MDLC, JGA, ALRM, MDSAL, and LMPB have \\nnothing to declare. RB received lecture fees from Abiogen, Faes Farma, \\nFresenius, and Proctor and Gamble outside the submitted work. JMQG received \\nlecture fees from FAES Farma (Spain) and Amgen related to vitamin D—these \\nactivities in no way influenced the writing of the present manuscript. JLM \\nreceived lecture fees from AMGEN, SANOFI, FERRER, Laboratorios Dr. Esteve, and \\nBoehringer Ingelheim-Lilly outside the submitted work. The funders had no role \\nin the design of the study; in the collection, analyses, or interpretation of \\ndata; in the writing of the manuscript, or in the decision to publish the \\nresults. #40. New Gener Comput. 2021;39(3-4):717-741. doi: 10.1007/s00354-021-00128-0. Epub\\n 2021 Jun 10.\\n\\nLeveraging Artificial Intelligence (AI) Capabilities for COVID-19 Containment.\\n\\nSurianarayanan C(1), Chelliah PR(2).\\n\\nAuthor information:\\n(1)Government Arts and Science College (Formerly Bharathidasan University \\nConstituent Arts and Science College), Affiliated to Bharathidasan University, \\nTiruchirappalli, Tamilnadu India.\\n(2)Site Reliability Engineering Division, Reliance Jio Platforms Ltd, Bangalore, \\nIndia.\\n\\nThe Coronavirus disease (COVID-19) is an infectious disease caused by the newly \\ndiscovered Severe Acute Respiratory Syndrome Coronavirus two (SARS-CoV-2). Most \\nof the people do not have the acquired immunity to fight this virus. There is no \\nspecific treatment or medicine to cure the disease. The effects of this disease \\nappear to vary from individual to individual, right from mild cough, fever to \\nrespiratory disease. It also leads to mortality in many people. As the virus has \\na very rapid transmission rate, the entire world is in distress. The control and \\nprevention of this disease has evolved as an urgent and critical issue to be \\naddressed through technological solutions. The Healthcare industry therefore \\nneeds support from the domain of artificial intelligence (AI). AI has the \\ninherent capability of imitating the human brain and assisting in \\ndecision-making support by automatically learning from input data. It can \\nprocess huge amounts of data quickly without getting tiresome and making errors. \\nAI technologies and tools significantly relieve the burden of healthcare \\nprofessionals. In this paper, we review the critical role of AI in responding to \\ndifferent research challenges around the COVID-19 crisis. A sample \\nimplementation of a powerful probabilistic machine learning (ML) algorithm for \\nassessment of risk levels of individuals is incorporated in this paper. Other \\npertinent application areas such as surveillance of people and hotspots, \\nmortality prediction, diagnosis, prognostic assistance, drug repurposing and \\ndiscovery of protein structure, and vaccine are presented. The paper also \\ndescribes various challenges that are associated with the implementation of \\nAI-based tools and solutions for practical use.\\n\\n© Ohmsha, Ltd. and Springer Japan KK, part of Springer Nature 2021.\\n\\nDOI: 10.1007/s00354-021-00128-0\\nPMCID: PMC8191724\\nPMID: 34131359\\n\\nConflict of interest statement: Conflict of interestOn behalf of all authors, \\nthe corresponding author states that there is no conflict of interest. #94. Eur Urol. 2020 Dec;78(6):775-776. doi: 10.1016/j.eururo.2020.09.031. Epub\\n2020  Sep 17.\\n\\nThe Emerging Role of Artificial Intelligence in the Fight Against COVID-19.\\n\\nGhose A(1), Roy S(2), Vasdev N(3), Olsburgh J(4), Dasgupta P(5).\\n\\nAuthor information:\\n(1)Medway NHS Foundation Trust, Gillingham, UK.\\n(2)Applied Intelligence, Accenture, London, UK.\\n(3)Department of Urology, Hertfordshire and Bedfordshire Urological Cancer \\nCentre, Lister Hospital Stevenage, School of Medicine and Life Sciences, \\nUniversity of Hertfordshire, Hatfield, UKE.\\n(4)Guy's and St. Thomas' NHS Foundation Trust, London, UK.\\n(5)Faculty of Life Sciences and Medicine, King's College London, London, UK. \\nElectronic address: prokar.dasgupta@kcl.ac.uk.\\n\\nThe coronavirus disease 2019 (COVID-19) pandemic has generated large volumes of \\nclinical data that can be an invaluable resource towards answering a number of \\nimportant questions for this and future pandemics. Artificial intelligence can \\nhave an important role in analysing such data to identify populations at higher \\nrisk of COVID-19-related urological pathologies and to suggest treatments that \\nblock viral entry into cells by interrupting the angiotensin-converting enzyme \\n2-transmembrane serine protease 2 (ACE2-TMPRSS2) pathway.\\n\\nCopyright © 2020 European Association of Urology. Published by Elsevier B.V. All \\nrights reserved.\\n\\nDOI: 10.1016/j.eururo.2020.09.031\\nPMCID: PMC7498248\\nPMID: 32994064 [Indexed for MEDLINE] #7. JAMA Intern Med. 2020 Sep 1;180(9):1156-1163. doi: \\n10.1001/jamainternmed.2020.2020.\\n\\nContact Tracing Assessment of COVID-19 Transmission Dynamics in Taiwan and Risk \\nat Different Exposure Periods Before and After Symptom Onset.\\n\\nCheng HY(1), Jian SW(1), Liu DP(1), Ng TC(2), Huang WT(3), Lin HH(2)(4); Taiwan \\nCOVID-19 Outbreak Investigation Team.\\n\\nAuthor information:\\n(1)Epidemic Intelligence Center, Taiwan Centers for Disease Control, Taipei, \\nTaiwan.\\n(2)Institute of Epidemiology and Preventive Medicine, National Taiwan University \\nCollege of Public Health, Taipei, Taiwan.\\n(3)Office of Preventive Medicine, Taiwan Centers for Disease Control, Taipei, \\nTaiwan.\\n(4)Global Health Program, National Taiwan University College of Public Health, \\nTaipei, Taiwan.\\n\\nErratum in\\n    JAMA Intern Med. 2020 Sep 1;180(9):1264.\\n\\nComment in\\n    JAMA Intern Med. 2020 Sep 1;180(9):1163-1164.\\n    JAMA Intern Med. 2020 Sep 1;180(9):1262.\\n\\nIMPORTANCE: The dynamics of coronavirus disease 2019 (COVID-19) transmissibility \\nare yet to be fully understood. Better understanding of the transmission \\ndynamics is important for the development and evaluation of effective control \\npolicies.\\nOBJECTIVE: To delineate the transmission dynamics of COVID-19 and evaluate the \\ntransmission risk at different exposure window periods before and after symptom \\nonset.\\nDESIGN, SETTING, AND PARTICIPANTS: This prospective case-ascertained study in \\nTaiwan included laboratory-confirmed cases of COVID-19 and their contacts. The \\nstudy period was from January 15 to March 18, 2020. All close contacts were \\nquarantined at home for 14 days after their last exposure to the index case. \\nDuring the quarantine period, any relevant symptoms (fever, cough, or other \\nrespiratory symptoms) of contacts triggered a COVID-19 test. The final follow-up \\ndate was April 2, 2020.\\nMAIN OUTCOMES AND MEASURES: Secondary clinical attack rate (considering \\nsymptomatic cases only) for different exposure time windows of the index cases \\nand for different exposure settings (such as household, family, and health \\ncare).\\nRESULTS: We enrolled 100 confirmed patients, with a median age of 44 years \\n(range, 11-88 years), including 44 men and 56 women. Among their 2761 close \\ncontacts, there were 22 paired index-secondary cases. The overall secondary \\nclinical attack rate was 0.7% (95% CI, 0.4%-1.0%). The attack rate was higher \\namong the 1818 contacts whose exposure to index cases started within 5 days of \\nsymptom onset (1.0% [95% CI, 0.6%-1.6%]) compared with those who were exposed \\nlater (0 cases from 852 contacts; 95% CI, 0%-0.4%). The 299 contacts with \\nexclusive presymptomatic exposures were also at risk (attack rate, 0.7% [95% CI, \\n0.2%-2.4%]). The attack rate was higher among household (4.6% [95% CI, \\n2.3%-9.3%]) and nonhousehold (5.3% [95% CI, 2.1%-12.8%]) family contacts than \\nthat in health care or other settings. The attack rates were higher among those \\naged 40 to 59 years (1.1% [95% CI, 0.6%-2.1%]) and those aged 60 years and older \\n(0.9% [95% CI, 0.3%-2.6%]).\\nCONCLUSIONS AND RELEVANCE: In this study, high transmissibility of COVID-19 \\nbefore and immediately after symptom onset suggests that finding and isolating \\nsymptomatic patients alone may not suffice to contain the epidemic, and more \\ngeneralized measures may be required, such as social distancing.\\n\\nDOI: 10.1001/jamainternmed.2020.2020\\nPMCID: PMC7195694\\nPMID: 32356867 [Indexed for MEDLINE]\\n\\nConflict of interest statement: Conflict of Interest Disclosures: None reported. #50. J Infect Dis. 2021 Oct 28;224(8):1362-1371. doi: 10.1093/infdis/jiab107.\\n\\nPersistent SARS-CoV-2 RNA Shedding Without Evidence of Infectiousness: A Cohort \\nStudy of Individuals With COVID-19.\\n\\nOwusu D(1)(2), Pomeroy MA(1)(2), Lewis NM(1)(3), Wadhwa A(1)(4), Yousaf \\nAR(1)(2), Whitaker B(2), Dietrich E(2), Hall AJ(2), Chu V(1)(2), Thornburg N(2), \\nChristensen K(5), Kiphibane T(6), Willardson S(7), Westergaard R(8), Dasu T(9), \\nPray IW(1)(8), Bhattacharyya S(9), Dunn A(3), Tate JE(2), Kirking HL(2), \\nMatanock A(2); Household Transmission Study Team.\\n\\nCollaborators: Duca LM, Rabold E, Gharpure R, Njuguna H, Dawson P, Conners EE, \\nFields V, Salvatore P, Marcenac P, Reses HE, Fajans M, Laws RL, Yin S, Ye D, \\nPevzner E, Battey K, Tran C, O'Hegarty M, Vuong J, Chancey RJ, Gregory CJ, Banks \\nM, Rispens J, Lester S, Mills L, Fry A, Nabity S, Freeman B, Buono S.\\n\\nAuthor information:\\n(1)Epidemic Intelligence Service, Centers for Disease Control and Prevention, \\nAtlanta, Georgia, USA.\\n(2)COVID-19 Response Team, Centers for Disease Control and Prevention, Atlanta, \\nGeorgia, USA.\\n(3)Utah Department of Health, Salt Lake City, Utah, USA.\\n(4)Laboratory Leadership Service, Centers for Disease Control and Prevention, \\nAtlanta, Georgia, USA.\\n(5)Utah Public Health Laboratory, Taylorsville, Utah, USA.\\n(6)Salt Lake County Health Department, Salt Lake City, Utah, USA.\\n(7)Davis County Health Department, Clearfield, Utah, USA.\\n(8)Wisconsin Department of Health Services, Madison, Wisconsin, USA.\\n(9)City of Milwaukee Health Department, Milwaukee, Wisconsin, USA.\\n\\nBACKGROUND: To better understand severe acute respiratory syndrome coronavirus 2 \\n(SARS-CoV-2) shedding and infectivity, we estimated SARS-CoV-2 RNA shedding \\nduration, described participant characteristics associated with the first \\nnegative rRT-PCR test (resolution), and determined if replication-competent \\nviruses was recoverable ≥10 days after symptom onset.\\nMETHODS: We collected serial nasopharyngeal specimens from 109 individuals with \\nrRT-PCR-confirmed COVID-19 in Utah and Wisconsin. We calculated viral RNA \\nshedding resolution probability using the Kaplan-Meier estimator and evaluated \\ncharacteristics associated with shedding resolution using Cox proportional \\nhazards regression. We attempted viral culture for 35 rRT-PCR-positive \\nnasopharyngeal specimens collected ≥10 days after symptom onset.\\nRESULTS: The likelihood of viral RNA shedding resolution at 10 days after \\nsymptom onset was approximately 3%. Time to shedding resolution was shorter \\namong participants aged <18 years (adjusted hazards ratio [aHR], 3.01; 95% \\nconfidence interval [CI], 1.6-5.6) and longer among those aged ≥50 years (aHR, \\n0.50; 95% CI, .3-.9) compared to participants aged 18-49 years. No \\nreplication-competent viruses were recovered.\\nCONCLUSIONS: Although most patients were positive for SARS-CoV-2 for ≥10 days \\nafter symptom onset, our findings suggest that individuals with mild to moderate \\nCOVID-19 are unlikely to be infectious ≥10 days after symptom onset.\\n\\nPublished by Oxford University Press for the Infectious Diseases Society of \\nAmerica 2021.\\n\\nDOI: 10.1093/infdis/jiab107\\nPMCID: PMC7989388\\nPMID: 33649773 [Indexed for MEDLINE] #76. Comput Electr Eng. 2022 Oct;103:108352. doi:\\n10.1016/j.compeleceng.2022.108352.  Epub 2022 Sep 2.\\n\\nAn AI-based disease detection and prevention scheme for COVID-19.\\n\\nTanwar S(1), Kumari A(1), Vekaria D(1), Kumar N(2)(3), Sharma R(4).\\n\\nAuthor information:\\n(1)Department of Computer Science and Engineering, Institute of Technology, \\nNirma University, Ahmedabad, India.\\n(2)Thapar Institute of Engineering and Technology, (Deemed to be University), \\nPatiala, Punjab, India.\\n(3)Department of Computer Science and Information Engineering, Asia University, \\nTaichung, Taiwan.\\n(4)Centre for Inter-Disciplinary Research and Innovation, University of \\nPetroleum and Energy Studies, P.O. Bidholi Via-Prem Nagar, Dehradun, India.\\n\\nThe proliferating outbreak of COVID-19 raises global health concerns and has \\nbrought many countries to a standstill. Several restrain strategies are imposed \\nto suppress and flatten the mortality curve, such as lockdowns, quarantines, \\netc. Artificial Intelligence (AI) techniques could be a promising solution to \\nleverage these restraint strategies. However, real-time decision-making \\nnecessitates a cloud-oriented AI solution to control the pandemic. Though many \\ncloud-oriented solutions exist, they have not been fully exploited for real-time \\ndata accessibility and high prediction accuracy. Motivated by these facts, this \\npaper proposes a cloud-oriented AI-based scheme referred to as D-espy (i.e., \\nDisease-espy) for disease detection and prevention. The proposed D-espy scheme \\nperforms a comparative analysis between Autoregressive Integrated Moving Average \\n(ARIMA), Vanilla Long Short Term Memory (LSTM), and Stacked LSTM techniques, \\nwhich signify the dominance of Stacked LSTM in terms of prediction accuracy. \\nThen, a Medical Resource Distribution (MRD) mechanism is proposed for the \\noptimal distribution of medical resources. Next, a three-phase analysis of the \\nCOVID-19 spread is presented, which can benefit the governing bodies in deciding \\nlockdown relaxation. Results show the efficacy of the D-espy scheme concerning \\n96.2% of prediction accuracy compared to the existing approaches.\\n\\n© 2022 Elsevier Ltd. All rights reserved.\\n\\nDOI: 10.1016/j.compeleceng.2022.108352\\nPMCID: PMC9436917\\nPMID: 36068837\\n\\nConflict of interest statement: No author associated with this paper has \\ndisclosed any potential or pertinent conflicts which may be perceived to have \\nimpending conflict with this work. For full disclosure statements refer to \\nhttps://doi.org/10.1016/j.compeleceng.2022.108352. #67. J Med Internet Res. 2021 Aug 6;23(8):e30200. doi: 10.2196/30200.\\n\\nExploring Changes to the Actionability of COVID-19 Dashboards Over the Course of \\n2020 in the Canadian Context: Descriptive Assessment and Expert Appraisal Study.\\n\\nBarbazza E(#)(1), Ivanković D(#)(1), Wang S(2)(3), Gilmore KJ(4), Poldrugovac \\nM(1), Willmington C(4), Larrain N(2)(3), Bos V(1), Allin S(5), Klazinga N(1), \\nKringos D(1).\\n\\nAuthor information:\\n(1)Department of Public and Occupational Health, Amsterdam UMC, Amsterdam Public \\nHealth Research Institute, University of Amsterdam, Amsterdam, Netherlands.\\n(2)OptiMedis AG, Hamburg, Germany.\\n(3)Hamburg Center for Health Economics, University of Hamburg, Hamburg, Germany.\\n(4)Laboratorio Management e Sanità, Institute of Management and Department \\nEMbeDS, Scuola Superiore Sant'Anna, Pisa, Italy.\\n(5)Institute of Health Policy, Management and Evaluation, University of Toronto, \\nToronto, ON, Canada.\\n(#)Contributed equally\\n\\nBACKGROUND: Public web-based COVID-19 dashboards are in use worldwide to \\ncommunicate pandemic-related information. Actionability of dashboards, as a \\npredictor of their potential use for data-driven decision-making, was assessed \\nin a global study during the early stages of the pandemic. It revealed a \\nwidespread lack of features needed to support actionability. In view of the \\ninherently dynamic nature of dashboards and their unprecedented speed of \\ncreation, the evolution of dashboards and changes to their actionability merit \\nexploration.\\nOBJECTIVE: We aimed to explore how COVID-19 dashboards evolved in the Canadian \\ncontext during 2020 and whether the presence of actionability features changed \\nover time.\\nMETHODS: We conducted a descriptive assessment of a pan-Canadian sample of \\nCOVID-19 dashboards (N=26), followed by an appraisal of changes to their \\nactionability by a panel of expert scorers (N=8). Scorers assessed the \\ndashboards at two points in time, July and November 2020, using an assessment \\ntool informed by communication theory and health care performance intelligence. \\nApplying the nominal group technique, scorers were grouped in panels of three, \\nand evaluated the presence of the seven defined features of highly actionable \\ndashboards at each time point.\\nRESULTS: Improvements had been made to the dashboards over time. These \\npredominantly involved data provision (specificity of geographic breakdowns, \\nrange of indicators reported, and explanations of data sources or calculations) \\nand advancements enabled by the technologies employed (customization of time \\ntrends and interactive or visual chart elements). Further improvements in \\nactionability were noted especially in features involving local-level data \\nprovision, time-trend reporting, and indicator management. No improvements were \\nfound in communicative elements (clarity of purpose and audience), while the use \\nof storytelling techniques to narrate trends remained largely absent from the \\ndashboards.\\nCONCLUSIONS: Improvements to COVID-19 dashboards in the Canadian context during \\n2020 were seen mostly in data availability and dashboard technology. Further \\nimproving the actionability of dashboards for public reporting will require \\nattention to both technical and organizational aspects of dashboard development. \\nSuch efforts would include better skill-mixing across disciplines, continued \\ninvestment in data standards, and clearer mandates for their developers to \\nensure accountability and the development of purpose-driven dashboards.\\n\\n©Erica Barbazza, Damir Ivanković, Sophie Wang, Kendall Jamieson Gilmore, Mircha \\nPoldrugovac, Claire Willmington, Nicolas Larrain, Véronique Bos, Sara Allin, \\nNiek Klazinga, Dionne Kringos. Originally published in the Journal of Medical \\nInternet Research (https://www.jmir.org), 06.08.2021.\\n\\nDOI: 10.2196/30200\\nPMCID: PMC8360335\\nPMID: 34280120 [Indexed for MEDLINE]\\n\\nConflict of interest statement: Conflicts of Interest: None declare. #21. Clin Infect Dis. 2021 Dec 16;73(12):2217-2225. doi: 10.1093/cid/ciab148.\\n\\nClinical and Laboratory Findings in Patients With Potential Severe Acute \\nRespiratory Syndrome Coronavirus 2 (SARS-CoV-2) Reinfection, May-July 2020.\\n\\nLee JT(1)(2), Hesse EM(1), Paulin HN(1), Datta D(1), Katz LS(1), Talwar A(1), \\nChang G(1), Galang RR(1), Harcourt JL(3), Tamin A(3), Thornburg NJ(3), Wong \\nKK(1), Stevens V(1), Kim K(1), Tong S(3), Zhou B(3), Queen K(3), Drobeniuc J(3), \\nFolster JM(3), Sexton DJ(3), Ramachandran S(3), Browne H(3), Iskander J(1), \\nMitruka K(1).\\n\\nAuthor information:\\n(1)Health Systems Worker Safety Task Force, COVID-19 Response, Centers for \\nDisease Control and Prevention, Atlanta, Georgia, USA.\\n(2)Epidemic Intelligence Service, Centers for Disease Control and Prevention, \\nAtlanta, Georgia, USA.\\n(3)Laboratory Task Force, COVID-19 Response, Centers for Disease Control and \\nPrevention, Atlanta, Georgia, USA.\\n\\nComment in\\n    Clin Infect Dis. 2021 Dec 16;73(12):2226-2227.\\n\\nBACKGROUND: We investigated patients with potential severe acute respiratory \\nsyndrome coronavirus 2 (SARS-CoV-2) reinfection in the United States during \\nMay-July 2020.\\nMETHODS: We conducted case finding for patients with potential SARS-CoV-2 \\nreinfection through the Emerging Infections Network. Cases reported were \\nscreened for laboratory and clinical findings of potential reinfection followed \\nby requests for medical records and laboratory specimens. Available medical \\nrecords were abstracted to characterize patient demographics, comorbidities, \\nclinical course, and laboratory test results. Submitted specimens underwent \\nfurther testing, including reverse transcription polymerase chain reaction \\n(RT-PCR), viral culture, whole genome sequencing, subgenomic RNA PCR, and \\ntesting for anti-SARS-CoV-2 total antibody.\\nRESULTS: Among 73 potential reinfection patients with available records, 30 \\npatients had recurrent coronavirus disease 2019 (COVID-19) symptoms explained by \\nalternative diagnoses with concurrent SARS-CoV-2 positive RT-PCR, 24 patients \\nremained asymptomatic after recovery but had recurrent or persistent RT-PCR, and \\n19 patients had recurrent COVID-19 symptoms with concurrent SARS-CoV-2 positive \\nRT-PCR but no alternative diagnoses. These 19 patients had symptom recurrence a \\nmedian of 57 days after initial symptom onset (interquartile range: 47-76). Six \\nof these patients had paired specimens available for further testing, but none \\nhad laboratory findings confirming reinfections. Testing of an additional 3 \\npatients with recurrent symptoms and alternative diagnoses also did not confirm \\nreinfection.\\nCONCLUSIONS: We did not confirm SARS-CoV-2 reinfection within 90 days of the \\ninitial infection based on the clinical and laboratory characteristics of cases \\nin this investigation. Our findings support current Centers for Disease Control \\nand Prevention (CDC) guidance around quarantine and testing for patients who \\nhave recovered from COVID-19.\\n\\n© The Author(s) 2021. Published by Oxford University Press for the Infectious \\nDiseases Society of America. All rights reserved. For permissions, e-mail: \\njournals.permissions@oup.com.\\n\\nDOI: 10.1093/cid/ciab148\\nPMCID: PMC7929000\\nPMID: 33598716 [Indexed for MEDLINE] #148. PeerJ Comput Sci. 2021 May 26;7:e564. doi: 10.7717/peerj-cs.564. eCollection\\n 2021.\\n\\nOverview of current state of research on the application of artificial \\nintelligence techniques for COVID-19.\\n\\nKumar V(1), Singh D(2), Kaur M(2), Damaševičius R(3)(4).\\n\\nAuthor information:\\n(1)Computer Science and Engineering Department, National Institute of \\nTechnology, Hamirpur, Himachal Pradesh, India.\\n(2)School of Engineering and Applied Sciences, Bennett University, Greater \\nNoida, India.\\n(3)Faculty of Applied Mathematics, Silesian University of Technology, Gliwice, \\nPoland.\\n(4)Department of Applied Informatics, Vytautas Magnus University, Kaunas, \\nLithuania.\\n\\nBACKGROUND: Until now, there are still a limited number of resources available \\nto predict and diagnose COVID-19 disease. The design of novel drug-drug \\ninteraction for COVID-19 patients is an open area of research. Also, the \\ndevelopment of the COVID-19 rapid testing kits is still a challenging task.\\nMETHODOLOGY: This review focuses on two prime challenges caused by urgent needs \\nto effectively address the challenges of the COVID-19 pandemic, i.e., the \\ndevelopment of COVID-19 classification tools and drug discovery models for \\nCOVID-19 infected patients with the help of artificial intelligence (AI) based \\ntechniques such as machine learning and deep learning models.\\nRESULTS: In this paper, various AI-based techniques are studied and evaluated by \\nthe means of applying these techniques for the prediction and diagnosis of \\nCOVID-19 disease. This study provides recommendations for future research and \\nfacilitates knowledge collection and formation on the application of the AI \\ntechniques for dealing with the COVID-19 epidemic and its consequences.\\nCONCLUSIONS: The AI techniques can be an effective tool to tackle the epidemic \\ncaused by COVID-19. These may be utilized in four main fields such as \\nprediction, diagnosis, drug design, and analyzing social implications for \\nCOVID-19 infected patients.\\n\\n© 2021 Kumar et al.\\n\\nDOI: 10.7717/peerj-cs.564\\nPMCID: PMC8176528\\nPMID: 34141890\\n\\nConflict of interest statement: Robertas Damasevicius is an Academic Editor for \\nPeerJ.\\n\\n\\n\\n\\nThe user's question: What is the best treatment for covid?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8950 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_response \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lm_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:838\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[0;32m--> 838\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
